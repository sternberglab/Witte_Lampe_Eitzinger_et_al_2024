{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import fnmatch\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SearchIO\n",
    "from Bio import motifs\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import re\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "from itertools import islice\n",
    "import shutil\n",
    "from Bio.Blast import NCBIXML\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing reads\n",
    "def import_fastq_files_current_directory(end_file):\n",
    "    sequence_vars = {}  # Dictionary to hold sequences with variable names as keys\n",
    "    sequence_var_names = []  # List to hold the variable names\n",
    "\n",
    "    # Get the current directory\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # Iterate over all files in the current directory\n",
    "    for filename in os.listdir(current_directory):\n",
    "        if filename.endswith(end_file):\n",
    "            # Extract the variable name by removing the \".fastq.gz\" extension\n",
    "            var_name = filename.replace(end_file, \"\")\n",
    "            \n",
    "            # Create the full file path\n",
    "            file_path = os.path.join(current_directory, filename)\n",
    "            \n",
    "            # Read the FASTQ file using SeqIO and store it in the variable\n",
    "            with gzip.open(file_path, \"rt\") as handle:\n",
    "                sequence_vars[var_name] = list(SeqIO.parse(handle, \"fastq\"))\n",
    "            \n",
    "            # Append the variable name to the list of sequence variable names\n",
    "            sequence_var_names.append(var_name)\n",
    "    \n",
    "    return sequence_vars, sequence_var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/glampe/Desktop/Columbia/Sternberg_Data/PACE_updates/20240903-indels/20240903-HTS-indelcalc/Edited-trimmed-alleles\")\n",
    "sequences, sequence_names = import_fastq_files_current_directory(\"_TnTrimmed_R1.fastq.gz\")\n",
    "os.chdir(\"/Users/glampe/Desktop/Columbia/Sternberg_Data/PACE_updates/20240903-indels/20240903-HTS-indelcalc/Edited-alleles\")\n",
    "sequences_untrimmed, sequence_names_untrimmed = import_fastq_files_current_directory(\"_edited_R1.fastq.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRL25-HEK-WT-Rep1 10 10\n",
      "TRL25-AAVS1-eeCAST-Rep2 6815 6815\n",
      "TRL25-hROSA26-eeCAST-Rep1 9418 9418\n",
      "TRL25-HEK-WT-Rep3 12 12\n",
      "TRL25-hROSA26-eeCAST-Rep3 12186 12186\n",
      "TRL25-AAVS1-WT-Rep2 358 358\n",
      "TRL25-ALB-WT-Rep1 45 45\n",
      "TRL25-hROSA26-WT-Rep1 150 150\n",
      "TRL25-ALB-eeCAST-Rep2 10805 10805\n",
      "TRL25-HEK-eeCAST-Rep2 1428 1428\n",
      "TRL25-hROSA26-WT-Rep3 234 234\n",
      "TRL25-ALB-WT-Rep3 92 92\n",
      "TRL25-AAVS1-eeCAST-Rep3 9061 9061\n",
      "TRL25-AAVS1-WT-Rep1 427 427\n",
      "TRL25-hROSA26-eeCAST-Rep2 11462 11462\n",
      "TRL25-AAVS1-eeCAST-Rep1 7954 7954\n",
      "TRL25-AAVS1-WT-Rep3 321 321\n",
      "TRL25-HEK-WT-Rep2 33 33\n",
      "TRL25-HEK-eeCAST-Rep1 1617 1617\n",
      "TRL25-ALB-eeCAST-Rep1 11716 11716\n",
      "TRL25-hROSA26-WT-Rep2 161 161\n",
      "TRL25-ALB-WT-Rep2 64 64\n",
      "TRL25-ALB-eeCAST-Rep3 13727 13727\n",
      "TRL25-HEK-eeCAST-Rep3 1844 1844\n"
     ]
    }
   ],
   "source": [
    "# checking input reads\n",
    "for item in sequence_names:\n",
    "    print(item, len(sequences[item]),len(sequences_untrimmed[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate qscore passing for minimum base call score\n",
    "def passes_quality_filter(seqrecord, threshold):\n",
    "    # Convert base qualities to Boolean based on Qscore threshold value. Only use reads with >=50% non-N:\n",
    "    # recordqual = [x > threshold for x in seqrecord.letter_annotations['phred_quality']]  # list of True, False etc\n",
    "    # return float(sum(recordqual)) / float(len(recordqual)) >= .5  # note that True = 1, False- = 0 for summing\n",
    "    # modified to just look for min q score present\n",
    "    return min(seqrecord.letter_annotations['phred_quality'])>=threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qscore filtering\n",
    "sequences_qscore = {}\n",
    "untrimmed_qscore = {}\n",
    "for item in sequence_names:\n",
    "    sequences_qscore[item]=[]\n",
    "    untrimmed_qscore[item]=[]\n",
    "    read_ids=[]\n",
    "    for sequence in sequences_untrimmed[item]:\n",
    "        if passes_quality_filter(sequence,20):\n",
    "            read_ids.append(sequence.id)\n",
    "            untrimmed_qscore[item].append(sequence)\n",
    "    for sequence in sequences[item]:\n",
    "        if sequence.id not in read_ids: continue\n",
    "        if passes_quality_filter(sequence,20):\n",
    "            sequences_qscore[item].append(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRL25-HEK-WT-Rep1 2 2\n",
      "TRL25-AAVS1-eeCAST-Rep2 1568 1568\n",
      "TRL25-hROSA26-eeCAST-Rep1 1096 1096\n",
      "TRL25-HEK-WT-Rep3 1 1\n",
      "TRL25-hROSA26-eeCAST-Rep3 1489 1489\n",
      "TRL25-AAVS1-WT-Rep2 83 83\n",
      "TRL25-ALB-WT-Rep1 21 21\n",
      "TRL25-hROSA26-WT-Rep1 17 17\n",
      "TRL25-ALB-eeCAST-Rep2 6754 6754\n",
      "TRL25-HEK-eeCAST-Rep2 418 418\n",
      "TRL25-hROSA26-WT-Rep3 29 29\n",
      "TRL25-ALB-WT-Rep3 56 56\n",
      "TRL25-AAVS1-eeCAST-Rep3 2072 2072\n",
      "TRL25-AAVS1-WT-Rep1 106 106\n",
      "TRL25-hROSA26-eeCAST-Rep2 1515 1515\n",
      "TRL25-AAVS1-eeCAST-Rep1 1855 1855\n",
      "TRL25-AAVS1-WT-Rep3 56 56\n",
      "TRL25-HEK-WT-Rep2 6 6\n",
      "TRL25-HEK-eeCAST-Rep1 494 494\n",
      "TRL25-ALB-eeCAST-Rep1 7373 7373\n",
      "TRL25-hROSA26-WT-Rep2 27 27\n",
      "TRL25-ALB-WT-Rep2 35 35\n",
      "TRL25-ALB-eeCAST-Rep3 8547 8547\n",
      "TRL25-HEK-eeCAST-Rep3 572 572\n"
     ]
    }
   ],
   "source": [
    "# check the amount of reads that passed qScore\n",
    "for item in sequence_names:\n",
    "    print(item, len(sequences_qscore[item]),len(untrimmed_qscore[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AAVS1': 'TGGGAAATGGAGTCC', 'ALB': 'TTATATTTATTTTCA', 'HEK': 'TTTCCTAGACAGGGG', 'hROSA26': 'GGAGCGGAAAATGCT'}\n"
     ]
    }
   ],
   "source": [
    "# generate reference sequences and the anchor sequences to start the comparison\n",
    "\n",
    "target_sequences = {\n",
    "    'AAVS1':'accgCCCCAAGGATCTCCCGGTCCCCGCCCGGCGTGCTGACGTCACGGCGCTGCCCCAGGGTGTGCTGGGCAGGTCGCGGGGAGCGCTGGGAAATGGAGTCCATTAGCAGAAGTGGCCCTTGGCCACTTCCAGGAGTCGCTGTGCCCCGATGCACACTGGGAAGTCCGCAGCTCCGAGGCGCCCAGTGGAAATCGCCAGATGAGGGCCTCCTCCGGGGAATGCTGGGAAATGGAGTCTACAGGCCGGAGGGGTGCCCCACGGCATACTAGGAAGTGTGTAGCACCGGGTAAAGGGGATG'.upper(),\n",
    "    'ALB':'CCCAAAGACCTATCCATTGCACTATGCTTTATTTAAAAACCACAAAACCTGTGCTGTTGATCTCATAAATAGAACTTGTATTTATATTTATTTTCATTTTAGTCTGTCTTCTTGGTTGCTGTTGATAGACACTAAAAGAGTATTAGATATTATCTAAGTTTGAATATAAGGCtataaatatttaataatttttaaaataGTATTCTTGGTAATTGAATTATTCTTCTGTTTAAAGGCAGAAGAAATAATTGAACATCATCCTGAGTTTTTCTGT'.upper(),\n",
    "    'HEK':'GCTTTTCCTAGACAGGGGCTAGTATGTGCAGCTCCTGCACCGGGATACTGGTTGACAAGTTTGGCTGGGCTGGAAGCCAGCACCTAGGGAGGTCCCTGGAAGGGGCCAGCCTCACCAGGAGAGGAGGGACCTGGCCCTTCAGGGTCGAGCTCAACAGAGGAAAAGATCTCAGGGCACCCAGAGCCCAGTGGCTTTCAGCACCTGCATGAAAATCAGAGATC'.upper(),\n",
    "    'hROSA26':'CGTGGGAAGTCGGGAACATAATGTTTGTTACGTTGGGAGGGAAAGGGGTGGCTGGATGCAGGCGGGAGGGAGGCCCGCCCTGCGGCAACCGGAGGGGGAGGGAGAAGGGAGCGGAAAATGCTCGAAACCGGACGGAGCCATTGCTCTCGCAGAGGGAGGAGCGCTTCCGGCTAGCCTCTTGTCGCCGATTGGCCGTTTCTCCTCCCGCCGTGTGTGAAAACACAAATGGCGTATTCTGGTTGGAGTAAAGCTCCTGTCAGTTACGCCGTC'.upper()\n",
    "}\n",
    "crRNAs = {\n",
    "    'AAVS1':'TGGGAAATGGAGTCCATTAG',\n",
    "    'ALB':'TTATATTTATTTTCATTTTA',\n",
    "    'HEK':'TTTCCTAGACAGGGGCTAGT',\n",
    "    'hROSA26':'GGAGCGGAAAATGCTCGAAA'\n",
    "}\n",
    "anchors = {}\n",
    "for item in crRNAs.keys():\n",
    "    anchors[item]=crRNAs[item][-20:-5]\n",
    "print(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rev function \n",
    "def reverse_complement_sequences(sequence_list):\n",
    "    reverse_complemented_list = []\n",
    "    \n",
    "    for record in sequence_list:\n",
    "        # Reverse complement the sequence\n",
    "        reverse_complemented_seq = record.seq.reverse_complement()\n",
    "        \n",
    "        # Create a new SeqRecord with the reverse complement sequence\n",
    "        reverse_complemented_record = SeqRecord(reverse_complemented_seq, \n",
    "                                                id=f\"{record.id}_revcomp\")\n",
    "        \n",
    "        # Append the reverse complemented record to the list\n",
    "        reverse_complemented_list.append(reverse_complemented_record)\n",
    "    \n",
    "    return reverse_complemented_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rev comp all the reads to deal with orientation of sequencing\n",
    "sequences_rev = {}\n",
    "untrimmed_rev = {}\n",
    "for item in sequence_names:\n",
    "    sequences_rev[item]=reverse_complement_sequences(sequences_qscore[item])\n",
    "    untrimmed_rev[item]=reverse_complement_sequences(untrimmed_qscore[item])\n",
    "#     print(sequences_rev[item][0])\n",
    "#     print(untrimmed_rev[item][0])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRL25-HEK-WT-Rep1 0 0\n",
      "TRL25-AAVS1-eeCAST-Rep2 1518 1518\n",
      "TRL25-hROSA26-eeCAST-Rep1 1093 1093\n",
      "TRL25-HEK-WT-Rep3 1 1\n",
      "TRL25-hROSA26-eeCAST-Rep3 1488 1488\n",
      "TRL25-AAVS1-WT-Rep2 64 64\n",
      "TRL25-ALB-WT-Rep1 21 21\n",
      "TRL25-hROSA26-WT-Rep1 17 17\n",
      "TRL25-ALB-eeCAST-Rep2 6741 6741\n",
      "TRL25-HEK-eeCAST-Rep2 416 416\n",
      "TRL25-hROSA26-WT-Rep3 29 29\n",
      "TRL25-ALB-WT-Rep3 55 55\n",
      "TRL25-AAVS1-eeCAST-Rep3 1993 1993\n",
      "TRL25-AAVS1-WT-Rep1 94 94\n",
      "TRL25-hROSA26-eeCAST-Rep2 1510 1510\n",
      "TRL25-AAVS1-eeCAST-Rep1 1814 1814\n",
      "TRL25-AAVS1-WT-Rep3 48 48\n",
      "TRL25-HEK-WT-Rep2 2 2\n",
      "TRL25-HEK-eeCAST-Rep1 487 487\n",
      "TRL25-ALB-eeCAST-Rep1 7356 7356\n",
      "TRL25-hROSA26-WT-Rep2 27 27\n",
      "TRL25-ALB-WT-Rep2 30 30\n",
      "TRL25-ALB-eeCAST-Rep3 8536 8536\n",
      "TRL25-HEK-eeCAST-Rep3 563 563\n"
     ]
    }
   ],
   "source": [
    "# filter for reads with the anchor and trim there to make it easy\n",
    "anchored_seq={}\n",
    "anchored_untrimmed={}\n",
    "for item in sequence_names:\n",
    "    anchored_seq[item]=[]\n",
    "    anchored_untrimmed[item]=[]\n",
    "    site = ''\n",
    "    for anchorsite in anchors.keys():\n",
    "        if anchorsite in item:\n",
    "            site = anchorsite\n",
    "            break\n",
    "#     print(item, site)\n",
    "    for record in sequences_rev[item]:\n",
    "        anchorbase=record.seq.find(anchors[site])\n",
    "        if anchorbase != -1:\n",
    "            trimmed_sequence = record.seq[anchorbase:]\n",
    "            trimmed_record = SeqRecord(trimmed_sequence, id=record.id)\n",
    "            anchored_seq[item].append(trimmed_record)\n",
    "\n",
    "    for record in untrimmed_rev[item]:\n",
    "        anchorbase=record.seq.find(anchors[site])\n",
    "        if anchorbase != -1:\n",
    "            trimmed_sequence = record.seq[anchorbase:]\n",
    "            trimmed_record = SeqRecord(trimmed_sequence, id=record.id)\n",
    "            anchored_untrimmed[item].append(trimmed_record)\n",
    "            \n",
    "    print(item, len(anchored_seq[item]), len(anchored_untrimmed[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AAVS1': Seq('TGGGAAATGGAGTCCATTAGCAGAAGTGGCCCTTGGCCACTTCCAGGAGTCGCT...ATG'), 'ALB': Seq('TTATATTTATTTTCATTTTAGTCTGTCTTCTTGGTTGCTGTTGATAGACACTAA...TGT'), 'HEK': Seq('TTTCCTAGACAGGGGCTAGTATGTGCAGCTCCTGCACCGGGATACTGGTTGACA...ATC'), 'hROSA26': Seq('GGAGCGGAAAATGCTCGAAACCGGACGGAGCCATTGCTCTCGCAGAGGGAGGAG...GTC')}\n"
     ]
    }
   ],
   "source": [
    "# need to trim the stock sequence too, then go base-by-base\n",
    "targets_trimmed={}\n",
    "\n",
    "for item in target_sequences.keys():\n",
    "    sequence = Seq(target_sequences[item])\n",
    "    anchorsite=sequence.find(anchors[item])\n",
    "    targets_trimmed[item]=sequence[anchorsite:]\n",
    "\n",
    "print(targets_trimmed)\n",
    "#     print(item, site)\n",
    "#     for record in sequences_rev[item]:\n",
    "#         anchorbase=record.seq.find(anchors[site])\n",
    "#         if anchorbase != -1:\n",
    "#             sequences_trimmed[item].append(record.seq[anchorbase:])\n",
    "\n",
    "#     print(item, len(sequences_trimmed[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRL25-HEK-WT-Rep1\n",
      "TRL25-AAVS1-eeCAST-Rep2\n",
      "TRL25-hROSA26-eeCAST-Rep1\n",
      "TRL25-HEK-WT-Rep3\n",
      "TRL25-hROSA26-eeCAST-Rep3\n",
      "TRL25-AAVS1-WT-Rep2\n",
      "TRL25-ALB-WT-Rep1\n",
      "TRL25-hROSA26-WT-Rep1\n",
      "TRL25-ALB-eeCAST-Rep2\n",
      "TRL25-HEK-eeCAST-Rep2\n",
      "TRL25-hROSA26-WT-Rep3\n",
      "TRL25-ALB-WT-Rep3\n",
      "TRL25-AAVS1-eeCAST-Rep3\n",
      "TRL25-AAVS1-WT-Rep1\n",
      "TRL25-hROSA26-eeCAST-Rep2\n",
      "TRL25-AAVS1-eeCAST-Rep1\n",
      "TRL25-AAVS1-WT-Rep3\n",
      "TRL25-HEK-WT-Rep2\n",
      "TRL25-HEK-eeCAST-Rep1\n",
      "TRL25-ALB-eeCAST-Rep1\n",
      "TRL25-hROSA26-WT-Rep2\n",
      "TRL25-ALB-WT-Rep2\n",
      "TRL25-ALB-eeCAST-Rep3\n",
      "TRL25-HEK-eeCAST-Rep3\n"
     ]
    }
   ],
   "source": [
    "# now time to go base by base, and count it as needed, need to fix the indexing to be right for the tn insertion\n",
    "# just go 10 bp in, that will account for tRL or tLR\n",
    "\n",
    "# UNIVERSAL tn sequence TGTCGCTGCA\n",
    "# read by read, each read has 3 things: \n",
    "# the upstream sequence before trimming\n",
    "# the sequence after trimming\n",
    "# wt reference sequence\n",
    "\n",
    "# dictionaries are: \n",
    "# anchored_seq={}\n",
    "# anchored_untrimmed={}\n",
    "\n",
    "# the upstream bases are indexed as 0-len(upstream)-15\n",
    "# insertion are counted 1 to 10\n",
    "\n",
    "base_list = ['A','T','C','G']\n",
    "\n",
    "tn_end = Seq('TGTCGCTGCA')\n",
    "\n",
    "read_counter = {}\n",
    "error_counter = {}\n",
    "mismatch_description = {}\n",
    "mismatch_from_donor = {}\n",
    "totals_for_mismatch = {}\n",
    "totals_for_donor = {}\n",
    "reverseflank = Seq('TGTTTT')# reverse, not rev comp, to just split right in the dict\n",
    "# position 5 will be revflank[5-1]\n",
    "\n",
    "#### Mismatch description design:\n",
    "# mismatch_description[sample] is dict of positions\n",
    "# each position is 3 incorrect bases followed by dictionary of observed values\n",
    "\n",
    "for item in sequence_names:\n",
    "    mismatch_description[item] = {}\n",
    "    mismatch_from_donor[item] = {}\n",
    "    totals_for_donor[item]={}\n",
    "    for position in [1,2,3,4,5]:\n",
    "        totals_for_donor[item][position]=0\n",
    "        mismatch_from_donor[item][position] = {}\n",
    "        for baseseq in base_list: mismatch_from_donor[item][position][baseseq]=0\n",
    "        \n",
    "    totals_for_mismatch[item] = {}\n",
    "    print(item)\n",
    "    for target_site in targets_trimmed:\n",
    "        if target_site in item:\n",
    "            target=target_site\n",
    "            break\n",
    "    counter = {}\n",
    "    depth_count = {}\n",
    "    if len(anchored_seq[item])<1: continue\n",
    "    read_counter[item]={}\n",
    "    error_counter[item]={}\n",
    "    for record in anchored_seq[item]:\n",
    "        upstream_distance=0-len(record.seq)\n",
    "        indexing=0 \n",
    "        index=upstream_distance\n",
    "        for base in record:\n",
    "            if indexing not in mismatch_description[item]:\n",
    "                bases = [basecall for basecall in base_list if basecall != targets_trimmed[target][indexing]]\n",
    "                mismatch_description[item][indexing]= {}\n",
    "                totals_for_mismatch[item][indexing] = 0\n",
    "                for baseseq in bases: mismatch_description[item][indexing][baseseq]=0\n",
    "            if index not in counter:\n",
    "                counter[index]=0\n",
    "                depth_count[index]=0\n",
    "\n",
    "            depth_count[index]+=1\n",
    "            if base != targets_trimmed[target][indexing]:\n",
    "                totals_for_mismatch[item][indexing] += 1\n",
    "\n",
    "                counter[index]+=1\n",
    "                mismatch_description[item][indexing][record[indexing]]+=1\n",
    "                upstream = -1*index\n",
    "                if upstream < 6:\n",
    "                    totals_for_donor[item][upstream]+=1\n",
    "                    mismatch_from_donor[item][upstream][base] +=1\n",
    "                \n",
    "            index+=1 \n",
    "            indexing+=1\n",
    "        for record1 in anchored_untrimmed[item]:\n",
    "            if record1.id != record.id: continue\n",
    "            index2=0\n",
    "            for base in record1:\n",
    "                if index2<indexing:\n",
    "                    index2+=1\n",
    "                    continue\n",
    "                position_counter = index+1\n",
    "                if position_counter not in counter:\n",
    "                    counter[position_counter]=0\n",
    "                    depth_count[position_counter]=0\n",
    "                depth_count[position_counter]+=1\n",
    "                if tn_end[index] != base:\n",
    "                    counter[position_counter]+=1\n",
    "\n",
    "                index2+=1\n",
    "                index+=1\n",
    "                if index2>(indexing+9): break\n",
    "\n",
    "                \n",
    "            \n",
    "    counter = dict(sorted(counter.items()))\n",
    "    depth_count = dict(sorted(depth_count.items()))\n",
    "    error_counter[item]=counter\n",
    "    read_counter[item]=depth_count\n",
    "    \n",
    "    \n",
    "#     print(counter)\n",
    "#     print(depth_count)\n",
    "#     break\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     error_counter[item]=counter\n",
    "#     read_counter[item]=depth_count\n",
    "# #     print(counter)\n",
    "# #     print(depth_count)\n",
    "# #     break\n",
    "# # dictionary, each base from 1 to n, then have each target site be it's own output sheet, have two counts, mismatches and total counts at that base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='evoCAST_junction_combined.xlsx'\n",
    "\n",
    "min_negative_value = min(min(error_counter[sample].keys()) for sample in error_counter.keys())  # Capture min negative key\n",
    "\n",
    "max_index = 10  # Set the maximum positive index\n",
    "\n",
    "with pd.ExcelWriter(file_name) as writer:\n",
    "    sheet_name=\"pooled\"\n",
    "    dataframe_dict = {\"Base\": list(range(min_negative_value, max_index + 1))}\n",
    "    for sample in error_counter.keys(): \n",
    "        sample_column = [error_counter[sample].get(i, 0) for i in range(min_negative_value, max_index + 1)]\n",
    "        dataframe_dict[sample] = sample_column\n",
    "\n",
    "    for sample in error_counter.keys():\n",
    "        sample_column = [read_counter[sample].get(i, 0) for i in range(min_negative_value, max_index + 1)]\n",
    "        dataframe_dict[f\"{sample}_read-depth\"] = sample_column\n",
    "\n",
    "    df = pd.DataFrame(dataframe_dict)\n",
    "    df.to_excel(writer, sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TRL25-HEK-WT-Rep1': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-AAVS1-eeCAST-Rep2': {1: 25, 2: 13, 3: 14, 4: 4, 5: 1}, 'TRL25-hROSA26-eeCAST-Rep1': {1: 4, 2: 1, 3: 2, 4: 2, 5: 0}, 'TRL25-HEK-WT-Rep3': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-hROSA26-eeCAST-Rep3': {1: 2, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-AAVS1-WT-Rep2': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-ALB-WT-Rep1': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-hROSA26-WT-Rep1': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-ALB-eeCAST-Rep2': {1: 131, 2: 57, 3: 99, 4: 14, 5: 10}, 'TRL25-HEK-eeCAST-Rep2': {1: 13, 2: 1, 3: 0, 4: 0, 5: 0}, 'TRL25-hROSA26-WT-Rep3': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-ALB-WT-Rep3': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-AAVS1-eeCAST-Rep3': {1: 13, 2: 8, 3: 9, 4: 8, 5: 2}, 'TRL25-AAVS1-WT-Rep1': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-hROSA26-eeCAST-Rep2': {1: 2, 2: 0, 3: 1, 4: 1, 5: 1}, 'TRL25-AAVS1-eeCAST-Rep1': {1: 31, 2: 20, 3: 17, 4: 6, 5: 6}, 'TRL25-AAVS1-WT-Rep3': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-HEK-WT-Rep2': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-HEK-eeCAST-Rep1': {1: 15, 2: 0, 3: 1, 4: 0, 5: 0}, 'TRL25-ALB-eeCAST-Rep1': {1: 157, 2: 79, 3: 79, 4: 35, 5: 10}, 'TRL25-hROSA26-WT-Rep2': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-ALB-WT-Rep2': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}, 'TRL25-ALB-eeCAST-Rep3': {1: 102, 2: 47, 3: 17, 4: 5, 5: 1}, 'TRL25-HEK-eeCAST-Rep3': {1: 12, 2: 0, 3: 5, 4: 0, 5: 0}}\n"
     ]
    }
   ],
   "source": [
    "# print(mismatch_description)\n",
    "print(totals_for_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables saved to output.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_mismatch_table(mismatch_description, totals_for_mismatch, output_file):\n",
    "    # Initialize the ExcelWriter object to write multiple sheets\n",
    "    with pd.ExcelWriter(f\"/Users/glampe/Desktop/Columbia/Sternberg_Data/PACE_updates/20240903-indels/20240903-HTS-indelcalc/{output_file}.xlsx\", engine='xlsxwriter') as writer:\n",
    "        # Iterate through each sample in mismatch_description\n",
    "        for sample_name, sample_data in mismatch_description.items():\n",
    "            # Process only samples that contain the word \"eeCAST\"\n",
    "            if \"eeCAST\" in sample_name:\n",
    "                # Initialize a list to hold rows of the table\n",
    "                table_data = []\n",
    "                \n",
    "                # Iterate through positions 30 to 55\n",
    "                for position in range(30, 56):\n",
    "                    # Initialize a row with the position number and default base counts\n",
    "                    row = {\n",
    "                        'Position': position,\n",
    "                        'A': sample_data.get(position, {}).get('A', 0),\n",
    "                        'T': sample_data.get(position, {}).get('T', 0),\n",
    "                        'C': sample_data.get(position, {}).get('C', 0),\n",
    "                        'G': sample_data.get(position, {}).get('G', 0),\n",
    "                        'Total': totals_for_mismatch.get(position, 0)  # Use totals_for_mismatch for total count\n",
    "                    }\n",
    "                    \n",
    "                    # Append the row to the table data\n",
    "                    table_data.append(row)\n",
    "                \n",
    "                # Convert the list of rows to a DataFrame\n",
    "                df = pd.DataFrame(table_data)\n",
    "                \n",
    "                # Write the DataFrame to a new sheet in the Excel file\n",
    "                df.to_excel(writer, sheet_name=sample_name, index=False)\n",
    "        \n",
    "    print(f\"All tables saved to {output_file}.xlsx\")\n",
    "\n",
    "# Example usage\n",
    "create_mismatch_table(mismatch_description, totals_for_mismatch, \"output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables saved to /Users/glampe/Desktop/Columbia/Sternberg_Data/PACE_updates/20240903-indels/20240903-HTS-indelcalc/output_grouped_normalized_ignore_wt.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def create_mismatch_table_normalized_ignore_wt(mismatch_description, totals_for_mismatch, output_file):\n",
    "    # Dictionary to store data for each sample (grouped by name before \"Rep#\")\n",
    "    grouped_data = {}\n",
    "\n",
    "    # Iterate through each sample in mismatch_description\n",
    "    for sample_name, sample_data in mismatch_description.items():\n",
    "        # Skip samples that contain \"WT\"\n",
    "        if \"WT\" in sample_name:\n",
    "            continue\n",
    "\n",
    "        # Extract the sample name without \"Rep#\" part using regex\n",
    "        base_sample_name = re.sub(r'-Rep\\d+', '', sample_name)\n",
    "\n",
    "        # Initialize or append to the grouped data for each sample\n",
    "        if base_sample_name not in grouped_data:\n",
    "            grouped_data[base_sample_name] = []\n",
    "\n",
    "        # Collect data for each position (1 to 5 for this example)\n",
    "        for position in range(1, 6):\n",
    "            # Get total for this position, default to 1 if missing or zero to avoid division by zero\n",
    "            total = totals_for_mismatch[sample_name].get(position, 1)\n",
    "\n",
    "            # Initialize a row with normalized base counts\n",
    "            row = {\n",
    "                'Sample': sample_name,\n",
    "                'Position': position,\n",
    "                'A': sample_data.get(position, {}).get('A', 0) / total if total > 0 else 0,\n",
    "                'T': sample_data.get(position, {}).get('T', 0) / total if total > 0 else 0,\n",
    "                'C': sample_data.get(position, {}).get('C', 0) / total if total > 0 else 0,\n",
    "                'G': sample_data.get(position, {}).get('G', 0) / total if total > 0 else 0,\n",
    "                'Total': total\n",
    "            }\n",
    "\n",
    "            # Append the row to the appropriate sample in grouped_data\n",
    "            grouped_data[base_sample_name].append(row)\n",
    "\n",
    "    # Initialize the ExcelWriter object to write multiple sheets\n",
    "    with pd.ExcelWriter(f\"{output_file}.xlsx\", engine='xlsxwriter') as writer:\n",
    "        # Write each sample (grouped by base name) into a separate sheet\n",
    "        for base_sample_name, table_data in grouped_data.items():\n",
    "            # Convert the list of rows to a DataFrame\n",
    "            df = pd.DataFrame(table_data)\n",
    "\n",
    "            # Write the DataFrame to a new sheet in the Excel file\n",
    "            df.to_excel(writer, sheet_name=base_sample_name, index=False)\n",
    "\n",
    "    print(f\"All tables saved to {output_file}.xlsx\")\n",
    "\n",
    "# Example usage\n",
    "create_mismatch_table_normalized_ignore_wt(mismatch_from_donor, totals_for_donor, \"/Users/glampe/Desktop/Columbia/Sternberg_Data/PACE_updates/20240903-indels/20240903-HTS-indelcalc/output_grouped_normalized_ignore_wt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
